import argparse
from collections import Counter

from transformers import BertTokenizer, GPT2Tokenizer
from typing import Dict, List, Set

import log
from ota import MODELS

logger = log.get_logger('root')


def load_vocab(path: str) -> List[str]:
    with open(path, 'r', encoding='utf8') as f:
        vocab = f.read().splitlines()
    return vocab


def load_vocab_with_counts(path: str, min_freq=-1, max_freq=-1) -> Dict[str, int]:
    logger.info('Loading vocab from {} with minimum count {}'.format(path, min_freq))
    vocab = Counter()

    with open(path, 'r', encoding='utf-8') as f:
        for line in f:
            word, count = line.split()
            if (min_freq <= 0 or int(count) >= min_freq) and (max_freq <= 0 or int(count) <= max_freq):
                vocab[word] = int(count)

    logger.info('The loaded vocab contains {} words'.format(len(vocab)))
    return vocab


def get_difference(vocab_path: str, model: str, model_cls: str) -> List[str]:
    """
    Returns the difference between the words in a vocabulary file and the words in the vocabulary of a given
    Transformer model.
    :param vocab_path: the path to the vocabulary file
    :param model: the path to the model
    :param model_cls: the model class (currently supported: "bert" or "roberta")
    :return: the difference as a list of words
    """
    vocab_with_counts = load_vocab_with_counts(vocab_path)
    vocab = set(vocab_with_counts.keys())

    model_cls, tokenizer_cls = MODELS[model_cls]
    tokenizer = tokenizer_cls.from_pretrained(model)

    if isinstance(tokenizer, BertTokenizer):
        model_vocab = tokenizer.vocab.keys()
    elif isinstance(tokenizer, GPT2Tokenizer):
        model_vocab = set(tokenizer.encoder.keys())
        model_vocab.update(w[1:] for w in model_vocab if w.startswith('Ä '))
    else:
        raise ValueError('Access to vocab is currently only implemented for BertTokenizer and GPT2Tokenizer')

    logger.info('Vocab sizes: file = {}, model = {}'.format(len(vocab), len(model_vocab)))
    vocab -= model_vocab
    logger.info('Size of vocab difference = {}'.format(len(vocab)))

    vocab = list(vocab)
    vocab.sort(key=lambda x: vocab_with_counts[x], reverse=True)
    return vocab


def split_vocab(path: str, parts: int):
    vocab = load_vocab(path)
    part_size = int(len(vocab) / parts) + 1

    logger.info("Splitting vocab with {} words into {} parts with size {}".format(len(vocab), parts, part_size))
    vocab_splitted = [vocab[part_size * i: part_size * (i + 1)] for i in range(parts)]
    assert sum(len(x) for x in vocab_splitted) == len(vocab)

    for idx, vocab_part in enumerate(vocab_splitted):
        logger.info("Vocab part {} has size {}".format(idx, len(vocab_part)))
        write_vocab(vocab_part, path + str(idx))


def write_vocab(lines: List[str], file: str) -> None:
    with open(file, 'w', encoding='utf8') as f:
        for line in lines:
            f.write(line + '\n')


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument('--input', type=str, required=False)
    parser.add_argument('--output', default=None, type=str, required=True)
    parser.add_argument('--model', default='bert-base-uncased', type=str)
    parser.add_argument('--model_cls', default='bert', type=str, choices=['bert', 'roberta'])
    parser.add_argument('--parts', default=0, type=int)
    args = parser.parse_args()

    vocab = get_difference(args.input, args.model, args.model_cls)
    write_vocab(vocab, args.output)

    if args.parts > 0:
        split_vocab(args.output, args.parts)


if __name__ == "__main__":
    main()
